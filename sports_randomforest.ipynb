{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sports_randomforest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYgdXUn7KwnCnQzyXU8L5T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Launcha10/sports_entertaiment_topic_analysis/blob/kawamura/sports_randomforest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ghTGrEik1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0dde06dd-a5c3-43fa-bc63-8d8b44fdf8eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3xMwx5iiulA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd6eff65-654a-4610-ab9f-6bdd8ad34c6b"
      },
      "source": [
        "!apt install aptitude"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "0 upgraded, 21 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 3,877 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Fetched 3,877 kB in 2s (2,035 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 134443 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DttzsbUiu9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "795516ee-7ce9-419f-db3c-0592a8ef3900"
      },
      "source": [
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.5)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.5)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n",
            "The following packages will be REMOVED:\n",
            "  libnvidia-common-430{u} \n",
            "0 packages upgraded, 11 newly installed, 1 to remove and 25 not upgraded.\n",
            "Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n",
            "Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.3 [184 kB]\n",
            "Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.3 [68.7 kB]\n",
            "Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.3 [22.1 kB]\n",
            "Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.3 MB in 3s (9,872 kB/s)\n",
            "(Reading database ... 134902 files and directories currently installed.)\n",
            "Removing libnvidia-common-430 (430.64-0ubuntu0~gpu18.04.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 134897 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up file (1:5.32-2ubuntu0.3) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "                            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sURf52zLivBL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "80a02c4d-e10b-49f4-f0ee-01a7fdedcbaf"
      },
      "source": [
        "!pip install mecab-python3==0.7"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mecab-python3==0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/e9/bbf5fc790a2bedd96fbaf47a84afa060bfb0b3e0217e5f64b32bd4bbad69/mecab-python3-0.7.tar.gz (41kB)\n",
            "\r\u001b[K     |███████▉                        | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 40kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python3: filename=mecab_python3-0.7-cp36-cp36m-linux_x86_64.whl size=155490 sha256=7c267ed2de95255c4d08364126acbe19a49d3c424525db52dff4f0482b603657\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/07/3a/5f22ccc9f381f3bc01fa023202061cd1e0e9af855292f005dd\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAIxmsD2ivEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "75330073-d642-4b2e-deec-2b8484ac1a69"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from gensim import corpora, matutils\n",
        "import MeCab\n",
        "\n",
        "DATA_DIR_PATH = '/content/drive/My Drive/sports_text/'\n",
        "DICTIONARY_FILE_NAME = 'sportsdic.txt'\n",
        "mecab = MeCab.Tagger('Owakati')\n",
        "\n",
        "\n",
        "def get_class_id(file_name):\n",
        "    '''\n",
        "    ファイル名から、クラスIDを決定する。\n",
        "    学習データを作るときに使っています。\n",
        "    '''\n",
        "    #print(\"file_nameだよ：\"+file_name)\n",
        "    dir_list = get_dir_list()\n",
        "    #print(\"dir_listだよ：\")\n",
        "    #print(dir_list)\n",
        "    \n",
        "    dir_name = next(filter(lambda x: x in file_name, dir_list),None)\n",
        "    \n",
        "    #print(\"dir_nameだよ：\")\n",
        "    #print(dir_name)\n",
        "    if dir_name:\n",
        "        return dir_list.index(dir_name)\n",
        "    return None\n",
        "def get_dir_list():\n",
        "    '''\n",
        "    ライブドアコーパスが./text/ の下にカテゴリ別にあるからそのカテゴリ一覧をとってるだけ\n",
        "    '''\n",
        "    tmp = os.listdir(DATA_DIR_PATH)\n",
        "    #print(tmp)\n",
        "    #print(\"----------------------\")\n",
        "    if tmp is None:\n",
        "        return None\n",
        "    return sorted([x for x in tmp if os.path.isdir(DATA_DIR_PATH + x)])\n",
        "\n",
        "\n",
        "def get_file_content(file_path):\n",
        "    '''\n",
        "    1つの記事を読み込み\n",
        "    '''\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        return ''.join(f.readlines()[1:])  # ライブドアコーパスが3行目から本文はじまってるから\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    '''\n",
        "    とりあえず形態素解析して名詞だけ取り出す感じにしてる\n",
        "    '''\n",
        "    node = mecab.parseToNode(text)\n",
        "    while node:\n",
        "        if node.feature.split(',')[0] == '名詞':\n",
        "            yield node.surface.lower()\n",
        "        node = node.next\n",
        "\n",
        "\n",
        "def check_stopwords(word):\n",
        "    '''\n",
        "    ストップワードだったらTrueを返す\n",
        "    '''\n",
        "    if re.search(r'^[0-9]+$', word):  # 数字だけ\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_words(contents):\n",
        "    '''\n",
        "    記事群のdictについて、形態素解析して返す\n",
        "    '''\n",
        "    ret = []\n",
        "    for k, content in contents.items():\n",
        "        ret.append(get_words_main(content))\n",
        "    return ret\n",
        "\n",
        "\n",
        "def get_words_main(content):\n",
        "    '''\n",
        "    一つの記事を形態素解析して返す\n",
        "    '''\n",
        "    return [token for token in tokenize(content) if not check_stopwords(token)]\n",
        "\n",
        "\n",
        "def filter_dictionary(dictionary):\n",
        "    '''\n",
        "    低頻度と高頻度のワードを除く感じで\n",
        "    '''\n",
        "    dictionary.filter_extremes(no_below=2, no_above=0.5)  # この数字はあとで変えるかも\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "def get_contents():\n",
        "    '''\n",
        "    livedoorニュースのすべての記事をdictでまとめておく\n",
        "    '''\n",
        "    dir_list = get_dir_list()\n",
        "    #print(dir_list)\n",
        "    #print(\"-------------------------\")\n",
        "\n",
        "    if dir_list is None:\n",
        "        return None\n",
        "\n",
        "    ret = {}\n",
        "    for dir_name in dir_list:\n",
        "        file_list = os.listdir(DATA_DIR_PATH + dir_name)\n",
        "        print(file_list)\n",
        "        #print(\"--------------------\")\n",
        "\n",
        "        if file_list is None:\n",
        "            continue\n",
        "        for file_name in file_list:\n",
        "            #if dir_name in file_name:  # LICENSE.txt とかを除くためです。。\n",
        "                ret[file_name] = get_file_content(DATA_DIR_PATH + dir_name + '/' + file_name)\n",
        "        #print(len(ret))    \n",
        "        #print(\"-------------------\")\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "def get_vector(dictionary, content):\n",
        "    '''\n",
        "    ある記事の特徴語カウント\n",
        "    '''\n",
        "    tmp = dictionary.doc2bow(get_words_main(content))\n",
        "    dense = list(matutils.corpus2dense([tmp], num_terms=len(dictionary)).T[0])\n",
        "    return dense\n",
        "\n",
        "\n",
        "def get_dictionary(create_flg=False, file_name=DICTIONARY_FILE_NAME):\n",
        "    '''\n",
        "    辞書を作る\n",
        "    '''\n",
        "    if create_flg or not os.path.exists(file_name):\n",
        "        # データ読み込み\n",
        "        contents = get_contents()\n",
        "        \n",
        "        # 形態素解析して名詞だけ取り出す\n",
        "        words = get_words(contents)\n",
        "        #print(\"retだよ\")\n",
        "        #print(words)\n",
        "        # 辞書作成、そのあとフィルタかける\n",
        "        dictionary = filter_dictionary(corpora.Dictionary(words))\n",
        "        #print(\"dicだよ\")\n",
        "        #print(dictionary)\n",
        "        # 保存しておく\n",
        "        if file_name is None:\n",
        "            sys.exit()\n",
        "        dictionary.save_as_text(file_name)\n",
        "\n",
        "    else:\n",
        "        # 通常はファイルから読み込むだけにする\n",
        "        dictionary = corpora.Dictionary.load_from_text(file_name)\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    get_dictionary(create_flg=True)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['baseball-text2.txt', 'baseball-text3.txt', 'baseball-text4.txt', 'baseball-text5.txt', 'baseball-text6.txt', 'baseball-text7.txt', 'baseball-text8.txt', 'baseball-text9.txt', 'baseball-text10.txt', 'baseball-text11.txt', 'baseball-text12.txt', 'baseball-text13.txt', 'baseball-text14.txt', 'baseball-text15.txt', 'baseball-text16.txt', 'baseball-text17.txt', 'baseball-text18.txt', 'baseball-text19.txt', 'baseball-text20.txt', 'baseball-text21.txt', 'baseball-text22.txt', 'baseball-text23.txt', 'baseball-text24.txt', 'baseball-text25.txt', 'baseball-text26.txt', 'baseball-text27.txt', 'baseball-text28.txt', 'baseball-text29.txt', 'baseball-text30.txt', 'baseball-text31.txt', 'baseball-text32.txt', 'baseball-text33.txt', 'baseball-text34.txt', 'baseball-text35.txt', 'baseball-text36.txt', 'baseball-text37.txt', 'baseball-text38.txt', 'baseball-text39.txt', 'baseball-text40.txt', 'baseball-text41.txt', 'baseball-text42.txt', 'baseball-text43.txt', 'baseball-text44.txt', 'baseball-text45.txt', 'baseball-text46.txt', 'baseball-text47.txt', 'baseball-text48.txt', 'baseball-text49.txt', 'baseball-text50.txt', 'baseball-text51.txt', 'baseball-text52.txt', 'baseball-text53.txt', 'baseball-text54.txt', 'baseball-text55.txt', 'baseball-text56.txt', 'baseball-text57.txt', 'baseball-text58.txt', 'baseball-text59.txt', 'baseball-text60.txt', 'baseball-text61.txt', 'baseball-text62.txt', 'baseball-text63.txt', 'baseball-text64.txt', 'baseball-text65.txt', 'baseball-text66.txt', 'baseball-text67.txt', 'baseball-text68.txt', 'baseball-text69.txt', 'baseball-text70.txt', 'baseball-text71.txt', 'baseball-text72.txt', 'baseball-text73.txt', 'baseball-text74.txt', 'baseball-text75.txt', 'baseball-text76.txt', 'baseball-text77.txt', 'baseball-text78.txt', 'baseball-text79.txt', 'baseball-text80.txt', 'baseball-text81.txt', 'baseball-text82.txt', 'baseball-text83.txt', 'baseball-text84.txt', 'baseball-text85.txt', 'baseball-text86.txt', 'baseball-text87.txt', 'baseball-text88.txt', 'baseball-text89.txt', 'baseball-text90.txt', 'baseball-text91.txt', 'baseball-text92.txt', 'baseball-text93.txt', 'baseball-text94.txt', 'baseball-text95.txt', 'baseball-text96.txt', 'baseball-text97.txt', 'baseball-text98.txt', 'baseball-text99.txt', 'baseball-text100.txt', 'baseball-text1.txt']\n",
            "['basket-text22.txt', 'basket-text23.txt', 'basket-text24.txt', 'basket-text25.txt', 'basket-text26.txt', 'basket-text27.txt', 'basket-text28.txt', 'basket-text29.txt', 'basket-text30.txt', 'basket-text3.txt', 'basket-text1.txt', 'basket-text4.txt', 'basket-text2.txt', 'basket-text5.txt', 'basket-text6.txt', 'basket-text7.txt', 'basket-text20.txt', 'basket-text8.txt', 'basket-text21.txt', 'basket-text9.txt', 'basket-text10.txt', 'basket-text11.txt', 'basket-text12.txt', 'basket-text13.txt', 'basket-text14.txt', 'basket-text15.txt', 'basket-text16.txt', 'basket-text17.txt', 'basket-text18.txt', 'basket-text19.txt']\n",
            "['golf-text1.txt', 'golf-text2.txt', 'golf-text18.txt', 'golf-text78.txt', 'golf-text33.txt', 'golf-text93.txt', 'golf-text63.txt', 'golf-text3.txt', 'golf-text48.txt', 'golf-text94.txt', 'golf-text34.txt', 'golf-text19.txt', 'golf-text49.txt', 'golf-text64.txt', 'golf-text79.txt', 'golf-text95.txt', 'golf-text35.txt', 'golf-text50.txt', 'golf-text80.txt', 'golf-text20.txt', 'golf-text65.txt', 'golf-text4.txt', 'golf-text51.txt', 'golf-text21.txt', 'golf-text5.txt', 'golf-text66.txt', 'golf-text36.txt', 'golf-text81.txt', 'golf-text52.txt', 'golf-text82.txt', 'golf-text37.txt', 'golf-text53.txt', 'golf-text83.txt', 'golf-text22.txt', 'golf-text38.txt', 'golf-text54.txt', 'golf-text84.txt', 'golf-text23.txt', 'golf-text39.txt', 'golf-text6.txt', 'golf-text55.txt', 'golf-text85.txt', 'golf-text24.txt', 'golf-text7.txt', 'golf-text40.txt', 'golf-text86.txt', 'golf-text25.txt', 'golf-text67.txt', 'golf-text8.txt', 'golf-text87.txt', 'golf-text68.txt', 'golf-text26.txt', 'golf-text9.txt', 'golf-text56.txt', 'golf-text88.txt', 'golf-text27.txt', 'golf-text69.txt', 'golf-text10.txt', 'golf-text89.txt', 'golf-text57.txt', 'golf-text28.txt', 'golf-text70.txt', 'golf-text11.txt', 'golf-text90.txt', 'golf-text58.txt', 'golf-text71.txt', 'golf-text29.txt', 'golf-text91.txt', 'golf-text59.txt', 'golf-text72.txt', 'golf-text30.txt', 'golf-text60.txt', 'golf-text92.txt', 'golf-text73.txt', 'golf-text12.txt', 'golf-text31.txt', 'golf-text61.txt', 'golf-text41.txt', 'golf-text13.txt', 'golf-text74.txt', 'golf-text32.txt', 'golf-text62.txt', 'golf-text42.txt', 'golf-text75.txt', 'golf-text14.txt', 'golf-text43.txt', 'golf-text76.txt', 'golf-text15.txt', 'golf-text44.txt', 'golf-text77.txt', 'golf-text16.txt', 'golf-text45.txt', 'golf-text17.txt', 'golf-text46.txt', 'golf-text47.txt']\n",
            "['judo-text1.txt', 'judo-text2.txt', 'judo-text3.txt', 'judo-text4.txt', 'judo-text5.txt', 'judo-text6.txt', 'judo-text7.txt', 'judo-text8.txt', 'judo-text9.txt', 'judo-text10.txt', 'judo-text11.txt', 'judo-text12.txt', 'judo-text13.txt', 'judo-text14.txt', 'judo-text15.txt', 'judo-text16.txt', 'judo-text17.txt', 'judo-text18.txt', 'judo-text19.txt', 'judo-text20.txt', 'judo-text21.txt', 'judo-text22.txt', 'judo-text23.txt', 'judo-text24.txt', 'judo-text25.txt', 'judo-text26.txt', 'judo-text27.txt', 'judo-text28.txt', 'judo-text29.txt', 'judo-text30.txt', 'judo-text31.txt', 'judo-text32.txt', 'judo-text33.txt', 'judo-text34.txt', 'judo-text35.txt', 'judo-text36.txt', 'judo-text37.txt', 'judo-text38.txt', 'judo-text39.txt', 'judo-text40.txt']\n",
            "['kendo-text1.txt', 'kendo-text2.txt', 'kendo-text3.txt', 'kendo-text4.txt', 'kendo-text5.txt', 'kendo-text6.txt', 'kendo-text7.txt', 'kendo-text8.txt', 'kendo-text9.txt', 'kendo-text10.txt', 'kendo-text11.txt', 'kendo-text12.txt', 'kendo-text13.txt', 'kendo-text14.txt', 'kendo-text15.txt', 'kendo-text16.txt', 'kendo-text17.txt', 'kendo-text18.txt']\n",
            "['rugby-text23.txt', 'rugby-text22.txt', 'rugby-text20.txt', 'rugby-text6.txt', 'rugby-text7.txt', 'rugby-text5.txt', 'rugby-text4.txt', 'rugby-text24.txt', 'rugby-text21.txt', 'rugby-text8.txt', 'rugby-text16.txt', 'rugby-text14.txt', 'rugby-text9.txt', 'rugby-text13.txt', 'rugby-text10.txt', 'rugby-text15.txt', 'rugby-text17.txt', 'rugby-text11.txt', 'rugby-text27.txt', 'rugby-text28.txt', 'rugby-text25.txt', 'rugby-text26.txt', 'rugby-text19.txt', 'rugby-text3.txt', 'rugby-text18.txt', 'rugby-text1.txt', 'rugby-text2.txt', 'rugby-text12.txt']\n",
            "['soccer-text31.txt', 'soccer-text32.txt', 'soccer-text33.txt', 'soccer-text34.txt', 'soccer-text35.txt', 'soccer-text36.txt', 'soccer-text37.txt', 'soccer-text38.txt', 'soccer-text39.txt', 'soccer-text40.txt', 'soccer-text41.txt', 'soccer-text42.txt', 'soccer-text43.txt', 'soccer-text44.txt', 'soccer-text45.txt', 'soccer-text46.txt', 'soccer-text47.txt', 'soccer-text48.txt', 'soccer-text49.txt', 'soccer-text50.txt', 'soccer-text51.txt', 'soccer-text52.txt', 'soccer-text53.txt', 'soccer-text54.txt', 'soccer-text55.txt', 'soccer-text56.txt', 'soccer-text57.txt', 'soccer-text58.txt', 'soccer-text59.txt', 'soccer-text60.txt', 'soccer-text61.txt', 'soccer-text62.txt', 'soccer-text63.txt', 'soccer-text64.txt', 'soccer-text65.txt', 'soccer-text66.txt', 'soccer-text67.txt', 'soccer-text68.txt', 'soccer-text69.txt', 'soccer-text70.txt', 'soccer-text71.txt', 'soccer-text72.txt', 'soccer-text73.txt', 'soccer-text74.txt', 'soccer-text75.txt', 'soccer-text76.txt', 'soccer-text77.txt', 'soccer-text78.txt', 'soccer-text79.txt', 'soccer-text80.txt', 'soccer-text81.txt', 'soccer-text82.txt', 'soccer-text83.txt', 'soccer-text84.txt', 'soccer-text85.txt', 'soccer-text86.txt', 'soccer-text87.txt', 'soccer-text88.txt', 'soccer-text89.txt', 'soccer-text90.txt', 'soccer-text91.txt', 'soccer-text92.txt', 'soccer-text93.txt', 'soccer-text94.txt', 'soccer-text95.txt', 'soccer-text96.txt', 'soccer-text97.txt', 'soccer-text98.txt', 'soccer-text99.txt', 'soccer-text100.txt', 'soccer-text1.txt', 'soccer-text4.txt', 'soccer-text3.txt', 'soccer-text5.txt', 'soccer-text2.txt', 'soccer-text21.txt', 'soccer-text6.txt', 'soccer-text22.txt', 'soccer-text7.txt', 'soccer-text23.txt', 'soccer-text8.txt', 'soccer-text24.txt', 'soccer-text9.txt', 'soccer-text25.txt', 'soccer-text10.txt', 'soccer-text26.txt', 'soccer-text11.txt', 'soccer-text27.txt', 'soccer-text12.txt', 'soccer-text28.txt', 'soccer-text13.txt', 'soccer-text29.txt', 'soccer-text14.txt', 'soccer-text30.txt', 'soccer-text15.txt', 'soccer-text16.txt', 'soccer-text17.txt', 'soccer-text18.txt', 'soccer-text19.txt', 'soccer-text20.txt']\n",
            "['sumo-text1.txt', 'sumo-text2.txt', 'sumo-text3.txt', 'sumo-text4.txt', 'sumo-text5.txt', 'sumo-text6.txt', 'sumo-text7.txt', 'sumo-text8.txt', 'sumo-text9.txt', 'sumo-text10.txt', 'sumo-text11.txt', 'sumo-text12.txt', 'sumo-text13.txt', 'sumo-text14.txt', 'sumo-text15.txt', 'sumo-text16.txt', 'sumo-text17.txt', 'sumo-text18.txt', 'sumo-text19.txt', 'sumo-text20.txt', 'sumo-text21.txt', 'sumo-text22.txt', 'sumo-text23.txt', 'sumo-text24.txt', 'sumo-text25.txt', 'sumo-text26.txt', 'sumo-text27.txt', 'sumo-text28.txt', 'sumo-text29.txt', 'sumo-text30.txt', 'sumo-text31.txt', 'sumo-text32.txt', 'sumo-text33.txt', 'sumo-text34.txt', 'sumo-text35.txt', 'sumo-text36.txt', 'sumo-text37.txt', 'sumo-text38.txt', 'sumo-text39.txt', 'sumo-text40.txt', 'sumo-text41.txt', 'sumo-text42.txt', 'sumo-text43.txt', 'sumo-text44.txt', 'sumo-text45.txt', 'sumo-text46.txt', 'sumo-text47.txt', 'sumo-text48.txt', 'sumo-text49.txt', 'sumo-text50.txt', 'sumo-text51.txt', 'sumo-text52.txt', 'sumo-text53.txt', 'sumo-text54.txt', 'sumo-text55.txt', 'sumo-text56.txt', 'sumo-text57.txt', 'sumo-text58.txt', 'sumo-text59.txt', 'sumo-text60.txt', 'sumo-text61.txt', 'sumo-text62.txt']\n",
            "['tennis-text12.txt', 'tennis-text16.txt', 'tennis-text10.txt', 'tennis-text17.txt', 'tennis-text8.txt', 'tennis-text9.txt', 'tennis-text13.txt', 'tennis-text15.txt', 'tennis-text14.txt', 'tennis-text7.txt', 'tennis-text11.txt', 'tennis-text4.txt', 'tennis-text5.txt', 'tennis-text6.txt', 'tennis-text3.txt', 'tennis-text21.txt', 'tennis-text19.txt', 'tennis-text18.txt', 'tennis-text20.txt', 'tennis-text1.txt', 'tennis-text2.txt']\n",
            "['volley-text1.txt', 'volley-text2.txt', 'volley-text3.txt', 'volley-text4.txt', 'volley-text5.txt', 'volley-text6.txt', 'volley-text7.txt', 'volley-text8.txt', 'volley-text9.txt', 'volley-text10.txt', 'volley-text11.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv9G6zSyjDdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "18886174-5b73-46e3-f303-30cadf006583"
      },
      "source": [
        "#scikit-learnのインストール\n",
        "pip install scikit-learn"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3V7Qvvu9Jeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d39f99e-4113-4146-aaaa-a8aeb014f8e2"
      },
      "source": [
        "import sklearn\n",
        "\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg8otheX9JnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "20903dc5-53d6-4a8e-9f57-9310a95fe7fc"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "#import corpus\n",
        "# 不要な警告を非表示にする\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 辞書の読み込み\n",
        "    dictionary = get_dictionary(create_flg=False)\n",
        "    print(dictionary)\n",
        "    # 記事の読み込み\n",
        "    contents = get_contents()\n",
        "\n",
        "    # 特徴抽出\n",
        "    data_train = []\n",
        "    label_train = []\n",
        "    for file_name, content in contents.items():\n",
        "\n",
        "        data_train.append(get_vector(dictionary, content))\n",
        "        a = label_train.append(get_class_id(file_name))\n",
        "    print(a)\n",
        "    print(data_train)\n",
        "    print(label_train)\n",
        "\n",
        "    # 分類器\n",
        "    estimator = RandomForestClassifier()\n",
        "\n",
        "    # 学習\n",
        "    estimator.fit(data_train, label_train)\n",
        "\n",
        "    # 学習したデータを予測にかけてみる（ズルなので正答率高くないとおかしい）\n",
        "    print(\"==== 学習データと予測データが一緒の場合\")\n",
        "    print(estimator.score(data_train, label_train))\n",
        "\n",
        "    # 学習データと試験データに分けてみる\n",
        "    data_train_s, data_test_s, label_train_s, label_test_s = train_test_split(data_train, label_train, test_size=0.5)\n",
        "\n",
        "    # 分類器をもう一度定義\n",
        "    estimator2 = RandomForestClassifier()\n",
        "\n",
        "    # 学習\n",
        "    estimator2.fit(data_train_s, label_train_s)\n",
        "    print(\"==== 学習データと予測データが違う場合\")\n",
        "    print(estimator2.score(data_test_s, label_test_s))\n",
        "    print(data_train_s)\n",
        "    print(label_train_s)\n",
        "\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(10988 unique tokens: ['―、―、―、―', '―、―、―、―、―', '、', 'あいさつ', 'あいつ']...)\n",
            "['baseball-text2.txt', 'baseball-text3.txt', 'baseball-text4.txt', 'baseball-text5.txt', 'baseball-text6.txt', 'baseball-text7.txt', 'baseball-text8.txt', 'baseball-text9.txt', 'baseball-text10.txt', 'baseball-text11.txt', 'baseball-text12.txt', 'baseball-text13.txt', 'baseball-text14.txt', 'baseball-text15.txt', 'baseball-text16.txt', 'baseball-text17.txt', 'baseball-text18.txt', 'baseball-text19.txt', 'baseball-text20.txt', 'baseball-text21.txt', 'baseball-text22.txt', 'baseball-text23.txt', 'baseball-text24.txt', 'baseball-text25.txt', 'baseball-text26.txt', 'baseball-text27.txt', 'baseball-text28.txt', 'baseball-text29.txt', 'baseball-text30.txt', 'baseball-text31.txt', 'baseball-text32.txt', 'baseball-text33.txt', 'baseball-text34.txt', 'baseball-text35.txt', 'baseball-text36.txt', 'baseball-text37.txt', 'baseball-text38.txt', 'baseball-text39.txt', 'baseball-text40.txt', 'baseball-text41.txt', 'baseball-text42.txt', 'baseball-text43.txt', 'baseball-text44.txt', 'baseball-text45.txt', 'baseball-text46.txt', 'baseball-text47.txt', 'baseball-text48.txt', 'baseball-text49.txt', 'baseball-text50.txt', 'baseball-text51.txt', 'baseball-text52.txt', 'baseball-text53.txt', 'baseball-text54.txt', 'baseball-text55.txt', 'baseball-text56.txt', 'baseball-text57.txt', 'baseball-text58.txt', 'baseball-text59.txt', 'baseball-text60.txt', 'baseball-text61.txt', 'baseball-text62.txt', 'baseball-text63.txt', 'baseball-text64.txt', 'baseball-text65.txt', 'baseball-text66.txt', 'baseball-text67.txt', 'baseball-text68.txt', 'baseball-text69.txt', 'baseball-text70.txt', 'baseball-text71.txt', 'baseball-text72.txt', 'baseball-text73.txt', 'baseball-text74.txt', 'baseball-text75.txt', 'baseball-text76.txt', 'baseball-text77.txt', 'baseball-text78.txt', 'baseball-text79.txt', 'baseball-text80.txt', 'baseball-text81.txt', 'baseball-text82.txt', 'baseball-text83.txt', 'baseball-text84.txt', 'baseball-text85.txt', 'baseball-text86.txt', 'baseball-text87.txt', 'baseball-text88.txt', 'baseball-text89.txt', 'baseball-text90.txt', 'baseball-text91.txt', 'baseball-text92.txt', 'baseball-text93.txt', 'baseball-text94.txt', 'baseball-text95.txt', 'baseball-text96.txt', 'baseball-text97.txt', 'baseball-text98.txt', 'baseball-text99.txt', 'baseball-text100.txt', 'baseball-text1.txt']\n",
            "['basket-text22.txt', 'basket-text23.txt', 'basket-text24.txt', 'basket-text25.txt', 'basket-text26.txt', 'basket-text27.txt', 'basket-text28.txt', 'basket-text29.txt', 'basket-text30.txt', 'basket-text3.txt', 'basket-text1.txt', 'basket-text4.txt', 'basket-text2.txt', 'basket-text5.txt', 'basket-text6.txt', 'basket-text7.txt', 'basket-text20.txt', 'basket-text8.txt', 'basket-text21.txt', 'basket-text9.txt', 'basket-text10.txt', 'basket-text11.txt', 'basket-text12.txt', 'basket-text13.txt', 'basket-text14.txt', 'basket-text15.txt', 'basket-text16.txt', 'basket-text17.txt', 'basket-text18.txt', 'basket-text19.txt']\n",
            "['golf-text1.txt', 'golf-text2.txt', 'golf-text18.txt', 'golf-text78.txt', 'golf-text33.txt', 'golf-text93.txt', 'golf-text63.txt', 'golf-text3.txt', 'golf-text48.txt', 'golf-text94.txt', 'golf-text34.txt', 'golf-text19.txt', 'golf-text49.txt', 'golf-text64.txt', 'golf-text79.txt', 'golf-text95.txt', 'golf-text35.txt', 'golf-text50.txt', 'golf-text80.txt', 'golf-text20.txt', 'golf-text65.txt', 'golf-text4.txt', 'golf-text51.txt', 'golf-text21.txt', 'golf-text5.txt', 'golf-text66.txt', 'golf-text36.txt', 'golf-text81.txt', 'golf-text52.txt', 'golf-text82.txt', 'golf-text37.txt', 'golf-text53.txt', 'golf-text83.txt', 'golf-text22.txt', 'golf-text38.txt', 'golf-text54.txt', 'golf-text84.txt', 'golf-text23.txt', 'golf-text39.txt', 'golf-text6.txt', 'golf-text55.txt', 'golf-text85.txt', 'golf-text24.txt', 'golf-text7.txt', 'golf-text40.txt', 'golf-text86.txt', 'golf-text25.txt', 'golf-text67.txt', 'golf-text8.txt', 'golf-text87.txt', 'golf-text68.txt', 'golf-text26.txt', 'golf-text9.txt', 'golf-text56.txt', 'golf-text88.txt', 'golf-text27.txt', 'golf-text69.txt', 'golf-text10.txt', 'golf-text89.txt', 'golf-text57.txt', 'golf-text28.txt', 'golf-text70.txt', 'golf-text11.txt', 'golf-text90.txt', 'golf-text58.txt', 'golf-text71.txt', 'golf-text29.txt', 'golf-text91.txt', 'golf-text59.txt', 'golf-text72.txt', 'golf-text30.txt', 'golf-text60.txt', 'golf-text92.txt', 'golf-text73.txt', 'golf-text12.txt', 'golf-text31.txt', 'golf-text61.txt', 'golf-text41.txt', 'golf-text13.txt', 'golf-text74.txt', 'golf-text32.txt', 'golf-text62.txt', 'golf-text42.txt', 'golf-text75.txt', 'golf-text14.txt', 'golf-text43.txt', 'golf-text76.txt', 'golf-text15.txt', 'golf-text44.txt', 'golf-text77.txt', 'golf-text16.txt', 'golf-text45.txt', 'golf-text17.txt', 'golf-text46.txt', 'golf-text47.txt']\n",
            "['judo-text1.txt', 'judo-text2.txt', 'judo-text3.txt', 'judo-text4.txt', 'judo-text5.txt', 'judo-text6.txt', 'judo-text7.txt', 'judo-text8.txt', 'judo-text9.txt', 'judo-text10.txt', 'judo-text11.txt', 'judo-text12.txt', 'judo-text13.txt', 'judo-text14.txt', 'judo-text15.txt', 'judo-text16.txt', 'judo-text17.txt', 'judo-text18.txt', 'judo-text19.txt', 'judo-text20.txt', 'judo-text21.txt', 'judo-text22.txt', 'judo-text23.txt', 'judo-text24.txt', 'judo-text25.txt', 'judo-text26.txt', 'judo-text27.txt', 'judo-text28.txt', 'judo-text29.txt', 'judo-text30.txt', 'judo-text31.txt', 'judo-text32.txt', 'judo-text33.txt', 'judo-text34.txt', 'judo-text35.txt', 'judo-text36.txt', 'judo-text37.txt', 'judo-text38.txt', 'judo-text39.txt', 'judo-text40.txt']\n",
            "['kendo-text1.txt', 'kendo-text2.txt', 'kendo-text3.txt', 'kendo-text4.txt', 'kendo-text5.txt', 'kendo-text6.txt', 'kendo-text7.txt', 'kendo-text8.txt', 'kendo-text9.txt', 'kendo-text10.txt', 'kendo-text11.txt', 'kendo-text12.txt', 'kendo-text13.txt', 'kendo-text14.txt', 'kendo-text15.txt', 'kendo-text16.txt', 'kendo-text17.txt', 'kendo-text18.txt']\n",
            "['rugby-text23.txt', 'rugby-text22.txt', 'rugby-text20.txt', 'rugby-text6.txt', 'rugby-text7.txt', 'rugby-text5.txt', 'rugby-text4.txt', 'rugby-text24.txt', 'rugby-text21.txt', 'rugby-text8.txt', 'rugby-text16.txt', 'rugby-text14.txt', 'rugby-text9.txt', 'rugby-text13.txt', 'rugby-text10.txt', 'rugby-text15.txt', 'rugby-text17.txt', 'rugby-text11.txt', 'rugby-text27.txt', 'rugby-text28.txt', 'rugby-text25.txt', 'rugby-text26.txt', 'rugby-text19.txt', 'rugby-text3.txt', 'rugby-text18.txt', 'rugby-text1.txt', 'rugby-text2.txt', 'rugby-text12.txt']\n",
            "['soccer-text31.txt', 'soccer-text32.txt', 'soccer-text33.txt', 'soccer-text34.txt', 'soccer-text35.txt', 'soccer-text36.txt', 'soccer-text37.txt', 'soccer-text38.txt', 'soccer-text39.txt', 'soccer-text40.txt', 'soccer-text41.txt', 'soccer-text42.txt', 'soccer-text43.txt', 'soccer-text44.txt', 'soccer-text45.txt', 'soccer-text46.txt', 'soccer-text47.txt', 'soccer-text48.txt', 'soccer-text49.txt', 'soccer-text50.txt', 'soccer-text51.txt', 'soccer-text52.txt', 'soccer-text53.txt', 'soccer-text54.txt', 'soccer-text55.txt', 'soccer-text56.txt', 'soccer-text57.txt', 'soccer-text58.txt', 'soccer-text59.txt', 'soccer-text60.txt', 'soccer-text61.txt', 'soccer-text62.txt', 'soccer-text63.txt', 'soccer-text64.txt', 'soccer-text65.txt', 'soccer-text66.txt', 'soccer-text67.txt', 'soccer-text68.txt', 'soccer-text69.txt', 'soccer-text70.txt', 'soccer-text71.txt', 'soccer-text72.txt', 'soccer-text73.txt', 'soccer-text74.txt', 'soccer-text75.txt', 'soccer-text76.txt', 'soccer-text77.txt', 'soccer-text78.txt', 'soccer-text79.txt', 'soccer-text80.txt', 'soccer-text81.txt', 'soccer-text82.txt', 'soccer-text83.txt', 'soccer-text84.txt', 'soccer-text85.txt', 'soccer-text86.txt', 'soccer-text87.txt', 'soccer-text88.txt', 'soccer-text89.txt', 'soccer-text90.txt', 'soccer-text91.txt', 'soccer-text92.txt', 'soccer-text93.txt', 'soccer-text94.txt', 'soccer-text95.txt', 'soccer-text96.txt', 'soccer-text97.txt', 'soccer-text98.txt', 'soccer-text99.txt', 'soccer-text100.txt', 'soccer-text1.txt', 'soccer-text4.txt', 'soccer-text3.txt', 'soccer-text5.txt', 'soccer-text2.txt', 'soccer-text21.txt', 'soccer-text6.txt', 'soccer-text22.txt', 'soccer-text7.txt', 'soccer-text23.txt', 'soccer-text8.txt', 'soccer-text24.txt', 'soccer-text9.txt', 'soccer-text25.txt', 'soccer-text10.txt', 'soccer-text26.txt', 'soccer-text11.txt', 'soccer-text27.txt', 'soccer-text12.txt', 'soccer-text28.txt', 'soccer-text13.txt', 'soccer-text29.txt', 'soccer-text14.txt', 'soccer-text30.txt', 'soccer-text15.txt', 'soccer-text16.txt', 'soccer-text17.txt', 'soccer-text18.txt', 'soccer-text19.txt', 'soccer-text20.txt']\n",
            "['sumo-text1.txt', 'sumo-text2.txt', 'sumo-text3.txt', 'sumo-text4.txt', 'sumo-text5.txt', 'sumo-text6.txt', 'sumo-text7.txt', 'sumo-text8.txt', 'sumo-text9.txt', 'sumo-text10.txt', 'sumo-text11.txt', 'sumo-text12.txt', 'sumo-text13.txt', 'sumo-text14.txt', 'sumo-text15.txt', 'sumo-text16.txt', 'sumo-text17.txt', 'sumo-text18.txt', 'sumo-text19.txt', 'sumo-text20.txt', 'sumo-text21.txt', 'sumo-text22.txt', 'sumo-text23.txt', 'sumo-text24.txt', 'sumo-text25.txt', 'sumo-text26.txt', 'sumo-text27.txt', 'sumo-text28.txt', 'sumo-text29.txt', 'sumo-text30.txt', 'sumo-text31.txt', 'sumo-text32.txt', 'sumo-text33.txt', 'sumo-text34.txt', 'sumo-text35.txt', 'sumo-text36.txt', 'sumo-text37.txt', 'sumo-text38.txt', 'sumo-text39.txt', 'sumo-text40.txt', 'sumo-text41.txt', 'sumo-text42.txt', 'sumo-text43.txt', 'sumo-text44.txt', 'sumo-text45.txt', 'sumo-text46.txt', 'sumo-text47.txt', 'sumo-text48.txt', 'sumo-text49.txt', 'sumo-text50.txt', 'sumo-text51.txt', 'sumo-text52.txt', 'sumo-text53.txt', 'sumo-text54.txt', 'sumo-text55.txt', 'sumo-text56.txt', 'sumo-text57.txt', 'sumo-text58.txt', 'sumo-text59.txt', 'sumo-text60.txt', 'sumo-text61.txt', 'sumo-text62.txt']\n",
            "['tennis-text12.txt', 'tennis-text16.txt', 'tennis-text10.txt', 'tennis-text17.txt', 'tennis-text8.txt', 'tennis-text9.txt', 'tennis-text13.txt', 'tennis-text15.txt', 'tennis-text14.txt', 'tennis-text7.txt', 'tennis-text11.txt', 'tennis-text4.txt', 'tennis-text5.txt', 'tennis-text6.txt', 'tennis-text3.txt', 'tennis-text21.txt', 'tennis-text19.txt', 'tennis-text18.txt', 'tennis-text20.txt', 'tennis-text1.txt', 'tennis-text2.txt']\n",
            "['volley-text1.txt', 'volley-text2.txt', 'volley-text3.txt', 'volley-text4.txt', 'volley-text5.txt', 'volley-text6.txt', 'volley-text7.txt', 'volley-text8.txt', 'volley-text9.txt', 'volley-text10.txt', 'volley-text11.txt']\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==== 学習データと予測データが一緒の場合\n",
            "0.9920792079207921\n",
            "==== 学習データと予測データが違う場合\n",
            "0.9881422924901185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1lXFpyh9Jt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejp-NKfTSo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from gensim import corpora, matutils\n",
        "import MeCab\n",
        "\n",
        "DATA_DIR_PATH = '/content/drive/My Drive/text/'\n",
        "DICTIONARY_FILE_NAME = 'livedoordic.txt'\n",
        "mecab = MeCab.Tagger('Owakati')\n",
        "\n",
        "\n",
        "def get_class_id(file_name):\n",
        "    '''\n",
        "    ファイル名から、クラスIDを決定する。\n",
        "    学習データを作るときに使っています。\n",
        "    '''\n",
        "    dir_list = get_dir_list()\n",
        "    dir_name = next(filter(lambda x: x in file_name, dir_list), None)\n",
        "    if dir_name:\n",
        "        return dir_list.index(dir_name)\n",
        "    return None\n",
        "def get_dir_list():\n",
        "    '''\n",
        "    ライブドアコーパスが./text/ の下にカテゴリ別にあるからそのカテゴリ一覧をとってるだけ\n",
        "    '''\n",
        "    tmp = os.listdir(DATA_DIR_PATH)\n",
        "    if tmp is None:\n",
        "        return None\n",
        "    return sorted([x for x in tmp if os.path.isdir(DATA_DIR_PATH + x)])\n",
        "\n",
        "\n",
        "def get_file_content(file_path):\n",
        "    '''\n",
        "    1つの記事を読み込み\n",
        "    '''\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        return ''.join(f.readlines()[2:])  # ライブドアコーパスが3行目から本文はじまってるから\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    '''\n",
        "    とりあえず形態素解析して名詞だけ取り出す感じにしてる\n",
        "    '''\n",
        "    node = mecab.parseToNode(text)\n",
        "    while node:\n",
        "        if node.feature.split(',')[0] == '名詞':\n",
        "            yield node.surface.lower()\n",
        "        node = node.next\n",
        "\n",
        "\n",
        "def check_stopwords(word):\n",
        "    '''\n",
        "    ストップワードだったらTrueを返す\n",
        "    '''\n",
        "    if re.search(r'^[0-9]+$', word):  # 数字だけ\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_words(contents):\n",
        "    '''\n",
        "    記事群のdictについて、形態素解析して返す\n",
        "    '''\n",
        "    ret = []\n",
        "    for k, content in contents.items():\n",
        "        ret.append(get_words_main(content))\n",
        "    return ret\n",
        "\n",
        "\n",
        "def get_words_main(content):\n",
        "    '''\n",
        "    一つの記事を形態素解析して返す\n",
        "    '''\n",
        "    return [token for token in tokenize(content) if not check_stopwords(token)]\n",
        "\n",
        "\n",
        "def filter_dictionary(dictionary):\n",
        "    '''\n",
        "    低頻度と高頻度のワードを除く感じで\n",
        "    '''\n",
        "    dictionary.filter_extremes(no_below=20, no_above=0.3)  # この数字はあとで変えるかも\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "def get_contents():\n",
        "    '''\n",
        "    livedoorニュースのすべての記事をdictでまとめておく\n",
        "    '''\n",
        "    dir_list = get_dir_list()\n",
        "\n",
        "    if dir_list is None:\n",
        "        return None\n",
        "\n",
        "    ret = {}\n",
        "    for dir_name in dir_list:\n",
        "        file_list = os.listdir(DATA_DIR_PATH + dir_name)\n",
        "\n",
        "        if file_list is None:\n",
        "            continue\n",
        "        for file_name in file_list:\n",
        "            if dir_name in file_name:  # LICENSE.txt とかを除くためです。。\n",
        "                ret[file_name] = get_file_content(DATA_DIR_PATH + dir_name + '/' + file_name)\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "def get_vector(dictionary, content):\n",
        "    '''\n",
        "    ある記事の特徴語カウント\n",
        "    '''\n",
        "    tmp = dictionary.doc2bow(get_words_main(content))\n",
        "    dense = list(matutils.corpus2dense([tmp], num_terms=len(dictionary)).T[0])\n",
        "    return dense\n",
        "\n",
        "\n",
        "def get_dictionary(create_flg=False, file_name=DICTIONARY_FILE_NAME):\n",
        "    '''\n",
        "    辞書を作る\n",
        "    '''\n",
        "    if create_flg or not os.path.exists(file_name):\n",
        "        # データ読み込み\n",
        "        contents = get_contents()\n",
        "        # 形態素解析して名詞だけ取り出す\n",
        "        words = get_words(contents)\n",
        "        # 辞書作成、そのあとフィルタかける\n",
        "        dictionary = filter_dictionary(corpora.Dictionary(words))\n",
        "        # 保存しておく\n",
        "        if file_name is None:\n",
        "            sys.exit()\n",
        "        dictionary.save_as_text(file_name)\n",
        "\n",
        "    else:\n",
        "        # 通常はファイルから読み込むだけにする\n",
        "        dictionary = corpora.Dictionary.load_from_text(file_name)\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    get_dictionary(create_flg=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8uPN1pA8-YJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}